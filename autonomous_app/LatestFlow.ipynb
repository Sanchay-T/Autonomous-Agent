{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:00:17.689165Z",
     "start_time": "2023-09-22T12:00:17.664846Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import List, Tuple\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import langchain\n",
    "import wandb\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.vectorstores import Chroma , Qdrant\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "# print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "doc_dir = os.path.join(\"documents\" , \"iteration_1\")\n",
    "\n",
    "vector_store_path = os.path.join(\"vector_store\")\n",
    "\n",
    "prompt_file_path = os.path.join(\"autonomous_app\" , \"chat_prompt.json\")\n",
    "\n",
    "langchain.llm_cache = SQLiteCache(database_path=\"llm_cache.db\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://fa1dffb4-23bf-4b57-8cc2-730c85ead277.us-east-1-0.aws.cloud.qdrant.io:6333\"\n",
    "api_key_q = \"q_l-IpY7Y2j4nVO4mfCM28HmKosSLvO8vZBbCRpq7hU-ffF1KlSXNQ\"\n",
    "\n",
    "\n",
    "\n",
    "# def log_prompt(prompt:dict, run:wandb.run):\n",
    "#     prompt_artifact = wandb.Artifact(name=\"chat_prompt\", type=\"prompt\")\n",
    "#     with prompt_artifact.new_file(\"prompt.json\") as f:\n",
    "#         f.write(json.dumps(prompt))\n",
    "#     run.log_artifact(prompt_artifact)\n",
    "#\n",
    "# def log_dataset(documents:List[Document], run:wandb.run):\n",
    "#     document_artifact = wandb.Artifact(name=\"documentation_dataset\", type=\"dataset\")\n",
    "#     with document_artifact.new_file(\"document.json\") as f:\n",
    "#         for document in documents:\n",
    "#             f.write(document.json() + \"\\n\")\n",
    "#     run.log_artifact(document_artifact)\n",
    "\n",
    "# def log_index(vector_store_dir:str, run:wandb.run):\n",
    "#     index_artifact = wandb.Artifact(name=\"vector_store\", type=\"search_index\")\n",
    "#     index_artifact.add_dir(vector_store_dir)\n",
    "#     run.log_artifact(index_artifact)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# def ingest_and_log_data(\n",
    "#         docs_dir: str = doc_dir,\n",
    "#         chunk_size: int = 600,\n",
    "#         chunk_overlap: int = 200,\n",
    "#         vector_store_path: str = vector_store_path,\n",
    "#         prompt_file_path: str = prompt_file_path,\n",
    "#         wandb_project: str = \"AI Agents Hackathon\",\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     Ingest documentation data, create a vector store, and log artifacts to W&B.\n",
    "#     Designed to be used within a Django context.\n",
    "#     \"\"\"\n",
    "#     run = wandb.init(project=wandb_project)  # Move the wandb initialization to this function\n",
    "#     # user_vector_store_path = os.path.join(vector_store_path, unique_user_key)\n",
    "#\n",
    "#\n",
    "#     # Ingest data\n",
    "#     documents = ingest_data(\n",
    "#         docs_dir=docs_dir,\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap,\n",
    "#         vector_store_path=vector_store_path,\n",
    "#         wandb_project=wandb_project,\n",
    "#         prompt_file=prompt_file_path,\n",
    "#     )\n",
    "#\n",
    "#     # Log data to wandb\n",
    "#     log_dataset(documents, run )\n",
    "#     log_index(vector_store_path, run)\n",
    "#\n",
    "#     with open(prompt_file_path, 'r') as f:\n",
    "#         prompt_data = json.load(f)\n",
    "#     log_prompt(prompt_data, run)\n",
    "#\n",
    "#     # Finish the wandb run\n",
    "#     run.finish()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efc1c9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['documents\\\\user_1.md']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Skip to main content\\n\\nClaim your spot on the [waitlist](https://paperspace-6894371.hs-\\nsites.com/paperspace-h100) for the NVIDIA H100 GPUs!\\n\\n[![Paperspace Docs](https://docs.paperspace.com/img/logo-light-theme.svg)\\n**DOCS**](https://docs.paperspace.com/)[Notebooks](https://docs.paperspace.com/gradient/notebooks/)[Machines](https://docs.paperspace.com/core/)[Deployments](https://docs.paperspace.com/gradient/deployments/)[Account](https://docs.paperspace.com/account-management/)\\n\\n`ctrl``K`\\n\\n[![](https://docs.paperspace.com/img/icon-account.svg)Account](https://docs.paperspace.com/account-management)\\n\\n[Sign in](https://console.paperspace.com/)[Sign\\nup](https://console.paperspace.com/signup)\\n\\n[![Paperspace Docs](https://docs.paperspace.com/img/logo-light-theme.svg) **DOCS**](https://docs.paperspace.com/)\\n\\n  * [Notebooks](https://docs.paperspace.com/gradient/notebooks/)\\n  * [Machines](https://docs.paperspace.com/core/)\\n  * [Deployments](https://docs.paperspace.com/gradient/deployments/)\\n  * [Account](https://docs.paperspace.com/account-management/)\\n\\n‚Üê Back to main menu\\n\\n  * Getting Started\\n\\n  * Gradient Platform\\n\\n    * Notebooks\\n\\n      * [Overview](https://docs.paperspace.com/gradient/notebooks/)\\n      * [Runtimes](https://docs.paperspace.com/gradient/notebooks/runtimes)\\n      * [Storage and datasets](https://docs.paperspace.com/gradient/notebooks/notebook-storage)\\n      * [Machines](https://docs.paperspace.com/gradient/notebooks/machines)\\n      * [Terminal](https://docs.paperspace.com/gradient/notebooks/terminal)\\n      * [Remote Jupyter Kernel](https://docs.paperspace.com/gradient/notebooks/notebooks-remote-kernel)\\n      * [Sharing](https://docs.paperspace.com/gradient/notebooks/sharing)\\n      * [TensorBoard](https://docs.paperspace.com/gradient/notebooks/tensorboard)\\n    * Workflows [beta]\\n\\n    * Deployments\\n\\n  * Gradient Resources\\n\\n  * CLI & SDK\\n\\n  * [üöÄ Run on Gradient](https://docs.paperspace.com/gradient/notebooks/run-on-gradient)\\n\\n[{/**/}Account](https://docs.paperspace.com/account-management)\\n\\n[{/**/}Changelog](https://updates.paperspace.com)\\n\\n[Contact Support](https://docs.paperspace.com/contact-support)\\n\\n  * [](https://docs.paperspace.com/)\\n  * Gradient Platform\\n  * Notebooks\\n  * Storage and datasets\\n\\nOn this page\\n\\n# Storage and datasets\\n\\nIn Gradient Notebooks, there is a file browser, shared persistent storage, and\\nGradient Datasets. This guide explains the full storage architecture of your\\nnotebook.\\n\\n## Introduction to the file architecture of Gradient Notebooks\\u200b\\n\\nEvery notebook in Gradient has a file management interface that looks like\\nthis:\\n\\n![The file manager for Gradient Notebooks lives in the left\\nsidebar.](https://docs.paperspace.com/assets/images/notebook-started-0a00613ac3f409b6616be8aad98b879a.png)\\n\\nThe file manager within the notebook does **not** represent the full file\\nstructure of the notebook.\\n\\nThe full file structure of a notebook is as follows:\\n\\n![This is the full representation of the file structure behind Gradient\\nNotebooks. Notice that the file manager in Gradient Notebooks is represented\\nby the yellow box titled { notebook IDE }.](https://docs.paperspace.com/assets/images/local-file-architecture-710291ebcbbec2ce6c2268447def4c06.png)\\n\\nHere are the main components:\\n\\n  *  **File manager** \\\\- Files available in the normal IDE sidebar. This corresponds to the directory located at `/notebooks`.\\n  *  **Storage** \\\\- Shared persistent storage directory accessible to your entire team on a specific cluster. Available at `/storage`. This is a method for sharing data across notebooks and users. In the case of the **Private Workspace** team, the `/storage` volume cannot be shared with other users.\\n  *  **Gradient Datasets** \\\\- Team and public datasets that you can mount in the IDE. Ideal for large amounts of data and for sharing. Public datasets include popular datasets that Gradient makes available out of the box such as [MNIST](http://yann.lecun.com/exdb/mnist/).\\n\\n## What is the file manager?\\u200b\\n\\nRefer to Introduction to the file structure of Gradient Notebooks to\\nunderstand the overall file architecture of Gradient Notebooks.\\n\\nFiles stored in the file manager are persisted across notebook sessions. This\\nis the same directory that is represented by the yellow box labeled `{\\nnotebook IDE }` in the previous section.\\n\\ncaution\\n\\nWithin the `/notebooks` directory, the folder name `checkpoints` is reserved\\nby Jupyter. Avoid using `checkpoints` as a directory name in order to avoid\\nany unexpected behavior.\\n\\n![Files in the notebook IDE file manager \\\\(pictured on the left side of the\\nimage\\\\) are available whenever a notebook is in the Running\\nstate.](https://docs.paperspace.com/assets/images/notebook-file-manager-4823ac255cc2e3a05c97897653833f80.png)\\n\\nThe notebook must be in the **Running** state to display files.\\n\\n### How to upload large files and folders to the file manager\\u200b\\n\\nTo upload a large number of files or a large amount of data, it is best to use\\ncommand-line libraries such as [curl](https://curl.se),\\n[Wget](https://www.gnu.org/software/wget/), or\\n[gdown](https://github.com/wkentaro/gdown).\\n\\nHere is an example of how to use Wget to download the [Stanford Dogs\\ndataset](http://vision.stanford.edu/aditya86/ImageNetDogs/) to our notebook:\\n\\n![An easy way to download a large dataset to a notebook is to use the wget\\ncommand.](https://docs.paperspace.com/assets/images/stanford-dogs-121f8efedac73ef8a7ce19c7cbb56e51.gif)\\n\\nThis command downloads the dataset to our current folder:\\n\\n    \\n    \\n    !wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar  \\n    \\n\\nThat\\'s all there is to it! We can also perform the same command from the\\nterminal if we are on the Pro or Growth subscription plans.\\n\\n#### Transferring files from Google Drive\\u200b\\n\\nFiles/folders in Google Drive can be brought into your notebook using `gdown`.\\n\\n  1. Through the notebook or terminal execute: `pip install gdown` to install and `pip install --upgrade gdown` to upgrade. Use a `!` before each command in the notebook.\\n  2. In the permissions settings of the files/folders you want to upload, set the permissions to ‚ÄúAnyone with the Link.‚Äù\\n  3. Obtain the `file id` by copying and extracting it from the file share link and use the following commands based on your needs.\\n\\n![Obtain the file id in the Google Drive share\\nlink.](https://docs.paperspace.com/assets/images/googledrivelink-abc4772e91de38173d9b63c929c90052.png)\\n\\n  * For bigger than 500 Mb files use: `gdown \"<file_ID>&confirm=t\"`\\n  * For smaller files `gdown <file_ID>`\\n  * For Folders `gdown https://drive.google.com/drive/folders/<file_ID> -O /tmp/folder --folder`\\n\\n### How to download files and folders to the file manager\\u200b\\n\\nTo download large files or folders from the notebook, we suggest you zip/tar\\nthe files first. You can do this from the notebook or terminal.\\n\\n  1. Compress the files/folders using the following command in a notebook code cell or the terminal. If you use the notebook make sure to add a `!` before each command.\\n\\n    1. tar\\n        \\n                cd /notebooks  \\n        tar -cf [filename].tar [file1] [file2]...  \\n        \\n\\n    2. zip\\n        \\n                cd /notebooks  \\n        zip -r [filename].zip [file1] [file2]...  \\n        \\n\\n  2. Refresh the file manager\\n\\n  3. Right click on the compressed file created\\n\\n  4. Select the option **Download**\\n\\nIf the files are in shared storage or a dataset, they can be downloaded by\\nmoving them into the file manager and following the steps shown above.\\n\\n## What is shared storage?\\u200b\\n\\nRefer to Introduction to the file structure of Gradient Notebooks to\\nunderstand the overall file architecture of Gradient Notebooks.\\n\\nData can be shared between users on a team and between notebooks that belong\\nto users on a team.\\n\\nAccess to shared persistent storage must be done through code, either via the\\nnotebook terminal or via a code cell within a notebook, as there is currently\\nno way to access shared persistent storage from the GUI.\\n\\nnote\\n\\nShared storage cannot be accessed cross cluster. As a result, data stored in\\n`/storage` on the Gradient cluster will not be accessible on the Graphcore\\ncluster.\\n\\n### How to access shared persistent storage from a notebook code cell\\u200b\\n\\nWe can access shared persistent storage from a code cell within a notebook\\nusing the `!` operator and issuing our bash commands on a single line\\nconnected with the `&&` operator.\\n\\nFor example, to create a new directory within our persistent `/storage`\\ndirectory, we\\'ll input the following:\\n\\n    \\n    \\n    !cd /storage && mkdir data && cd data  \\n    \\n\\nThis is what that would look like in a notebook code cell:\\n\\n![Access shared persistent storage using the ! operator and entering bash\\ncommands on a single line with the &amp;&amp; operator.](https://docs.paperspace.com/assets/images/slash-storage-44e5eb2a54ca33f3aa0e5ce342d8c256.gif)\\n\\nWe can also access persistent storage via the terminal, as described in the\\nnext section.\\n\\n### How to access shared persistent storage from a notebook terminal\\u200b\\n\\nThe terminal feature requires Gradient Pro or Gradient Growth subscriptions.\\n\\nTo access persistent storage in a Gradient Notebooks terminal, we can use the\\n`cd` command to change into the persistent directory `/storage`.\\n\\nLet\\'s say we\\'d like to create a new persistent directory called `data`. We can\\naccomplish this as follows:\\n\\n    \\n    \\n    cd /storage  \\n    mkdir data  \\n    cd data  \\n    \\n\\nLet\\'s try it out:\\n\\n![Here we use the terminal to create a new shared persistent storage directory\\nlocated at /storage/data.](https://docs.paperspace.com/assets/images/terminal-storage-759f79b1c3726e9cc729fd283abd3456.gif)\\n\\nWe can now use the directory located at `/storage/data` to store any files we\\nneed to access across users and notebooks.\\n\\n### How to view storage limits\\u200b\\n\\nStorage in Gradient is scoped to the team level. By default, storage tiers are\\nas follows:\\n\\n| Free| Pro| Growth| Enterprise  \\n---|---|---|---|---  \\nStorage| 5 GB| 15 GB| 50 GB| ‚àû GB  \\n  \\nExcess storage is billed at $0.29 per GB per month and this is prorated for\\nthe duration of the month.\\n\\nAs an example, if we are on the Pro plan, which grants us 15 GB of storage,\\nand we use 50 GB of storage for an entire month, we will be billed (50 - 15) *\\n0.29 = $10.15 on top of our normal bill.\\n\\nTo view storage utilization, visit the **Storage** tab in the workspace\\nsettings.\\n\\nHere we have an example of the **Storage** tab for a new team that is not yet\\nusing any volume storage:\\n\\n![A new team that has yet to upload data will have nothing to display in the\\nStorage tab in team settings.](https://docs.paperspace.com/assets/images/empty-storage-example-80ea2bc7b391fd35d607f4cd2c181faa.png)\\n\\nHere we have an example of a Private Workspace team that is using a good\\namount of storage:\\n\\n![A team that has uploaded data will see a summary of storage volumes in the\\nStorage tab.](https://docs.paperspace.com/assets/images/storage-example-8e7fefc63f426be5edbff0638d0a70f0.png)\\n\\nIf we expect to be billed for storage overages, we can use the **Utilization**\\ntab to explore our storage use further.\\n\\nUse the file management tab to upload data, organize files and folders, and\\ndownload files stored in a notebook.\\n\\nSome additional options such as renaming, duplicating, and deleting files and\\nfolders are available by clicking the menu icon on the individual entity.\\n\\n![A number of file and folder management options are available in the Files\\nsidebar.](https://docs.paperspace.com/assets/images/file-management-options-ba85cc0d826fbc5a44fb0f415b0dcb92.png)\\n\\nThere are multiple ways to upload files to a notebook, which are discussed in\\nthe following sections.\\n\\n## What is a dataset?\\u200b\\n\\nRefer to Introduction to the file structure of Gradient Notebooks to\\nunderstand the overall file architecture of Gradient Notebooks.\\n\\nGradient Datasets are available as a first-class resource within Gradient\\nNotebooks.\\n\\n### How to mount datasets in a notebook\\u200b\\n\\nThe IDE supports mounting Gradient Datasets to explore data and train models.\\nUse the datasets tab to mount existing team datasets, mount public datasets,\\nand create new team datasets.\\n\\nMounting a dataset is as easy as clicking the MOUNT button next to either the\\nteam or public dataset you would like to use.\\n\\n![Mount a public dataset](https://docs.paperspace.com/assets/images/mount-public-dataset-5ad4d25bca8844329aad46f78f033f94.gif)\\n\\nWhen mounting a team dataset, this will only mount the `latest` version of a\\ndataset. To change the version of the dataset please see the Advanced Settings\\nsection below.\\n\\n## How to add small datasets to a notebook\\u200b\\n\\nTo add a new dataset, click on the + icon. Then name, describe and upload the\\ndata. Feel free to close the modal once you start the upload, this process is\\nstill happening in the background.\\n\\n![Upload some images from Stanford Dogs dataset](https://docs.paperspace.com/assets/images/uploading-files-to-dataset-b69a1d40e1981eb19b98c5abcc281442.gif)\\n\\nDatasets can also be added from the Gradient Project level. To learn more see\\n[this article](https://docs.paperspace.com/gradient/data/#how-to-create-a-dataset-and-dataset-version-in-the-cli).\\n\\n### How to add large datasets (5GB +) to a notebook\\u200b\\n\\nTo create datasets larger than 5GB, you can use the CLI through the terminal.\\nTo learn more about how to create a dataset through the CLI see [this\\narticle](https://docs.paperspace.com/gradient/data/#how-to-create-a-dataset-and-dataset-version-in-the-cli)\\n\\n### Datasets Advanced Settings\\u200b\\n\\nTo access the settings file that manages all of your mounted Datasets go to\\n`.gradient/settings.yaml`. Here you can see all of the mounted Datasets and\\ntheir arguments. This file should only be used to do one of the following:\\n\\n  1. Change the `version-id` of the dataset that should be mounted.\\n\\n    \\n    \\n    integrations:  \\n      quarterly-reports: # mounts in /datasets/quarterly-reports  \\n        type: dataset # denotes a paperspace dataset  \\n        id: dataset-id # a paperspace dataset id  \\n        version: verion-id # a paperspace version id  \\n      my-bucket-data: # mounts in /datasets/my-bucket-data  \\n        type: s3 # an s3 bucket  \\n        url: s3://my-bucket/my-data # your s3 bucket url  \\n        accessKeyId: AK123 # your s3 access key id  \\n        secretAccessKey: secret:my-bucket-secret-key # a paperspace secret with your s3 secret key  \\n        region: \"us-west-1\" # the aws region your bucket is in, if not in aws set \"endpoint\"  \\n        endpoint: \"https://my-bucket-host.com\" # a custom bucket host, do not set region if set  \\n    \\n\\n## Other Data Sources\\u200b\\n\\n### DigitalOcean Spaces\\u200b\\n\\nThe Gradient IDE provides the ability to mount DigitalOcean Spaces into\\nNotebooks to access data that is stored externally. This is available to Pro\\nand Growth plans. Follow these simple steps to mount your DigitalOcean Space.\\n\\n  1. Add a new data source and select the DigitalOcean Spaces icon.\\n  2. Enter the endpoint url (e.g. `https://jane.nyc3.digitaloceanspaces.com`), display name (an arbitrary name for the data source), along with the access key & secret.\\n    * note: you will have to upload project secrets to the under the project settings tab.\\n\\n![DigitalOcean Spaces data source](https://docs.paperspace.com/assets/images/do-data-source-b78516e544a1e4bedaf50b819a361eed.png)\\n\\nOnce the data source is created, find the source in the list of data sources\\nand click the mount button.\\n\\nThis will create a bidrectional mount on the underlying container for reading\\nand writing data to the space.\\n\\n### AWS S3\\u200b\\n\\nThe Gradient IDE provides the ability to mount public and private S3 buckets\\ninto the Notebook to access data that is stored externally. This is available\\nto Pro and Growth plans. Follow these simple steps to mount your S3.\\n\\n  1. Add a new data source and select the Amazon S3 icon.\\n  2. Enter the name of your datasource and bucket url. \\n  3. If the bucket is private add an Access Key ID and Secret Access Key by choosing a Gradient Secret in the dropdown. [Learn more](https://docs.paperspace.com/gradient/secrets/) about how to create a Gradient secret. \\n  4. The data source can now be mounted to your notebook and accessed through the data source panel.\\n\\n![This is how you mount an AWS S3 data source into the\\nIDE](https://docs.paperspace.com/assets/images/s3-data-source-mount-8490fb3e309a91bfe76a125cd2bc17d7.gif)\\n\\n#### S3-Compatible Data Source\\u200b\\n\\nTo connect to other S3-compatible data sources like Google Cloud Platform\\n(GCP), follow these steps.\\n\\n  1. Add the S3-compatible bucket url. For GCP it would look like `s3://example-bucket-name`\\n  2. Open the Advanced Settings and change the default endpoint. For GCP enter `https://storage.googleapis.com`\\n\\n![This is how you mount an S3-compatible data source into the\\nIDE](https://docs.paperspace.com/assets/images/s3-compatible-data-source-mount-26bd6b54518904d74048874a17c29e01.png)\\n\\n### Making an S3 Bucket Publicly Accessible\\u200b\\n\\n ** _Warning: This will allow anyone on the internet to access your files. DO\\nNOT enable this if you have sensitive information in your S3 bucket._**\\n\\nTo make an AWS S3 bucket publicly accessible without credentials, you\\'ll need\\nto update two settings in your bucket settings under the Permissions tab:\\n\\n  1. Uncheck \"Block all public access\".\\n\\n![Unchecked &quot;Block all public access&quot;\\ncheckbox.](https://docs.paperspace.com/assets/images/block-public-access-ca72ceb5e18a98b4fa279033a08bc324.png)\\n\\n  2. Edit ACL to allow Everyone (public access) List Objects and Read Bucket ACL.\\n\\n![Checked Everyone \\\\(public access\\\\) List Objects and Read Bucket ACL\\ncheckboxes.](https://docs.paperspace.com/assets/images/block-public-access-ca72ceb5e18a98b4fa279033a08bc324.png)\\n\\n## Storage uses and billing\\u200b\\n\\nGradient Notebooks provide volume storage and bucket storage. The delineation\\nrefers to whether the data is available online or offline and helps users pay\\nonly for what they use.\\n\\nWith volume storage, data is available only while running a Gradient Resource\\nsuch as a notebook or workflow. With bucket storage, data is available for\\nonline or offline viewing.\\n\\n### Volume storage\\u200b\\n\\nVolumes are persistent storage resources that provide shared access to a\\nfilesystem while the instance is online.\\n\\nExamples of volume storage include:\\n\\n  *  **Gradient Notebooks:** Any information stored in `/storage` and in `/notebooks`\\n  *  **Gradient Datasets:** Any dataset cache in Gradient Workflows or Gradient Deployments\\n\\nFor more information about team volumes go to the Storage tab in the Team\\nSettings view which can be found by clicking the user icon in the top right.\\n\\n![Visit Team settings &gt; Storage to view info on storage\\nvolumes.](https://docs.paperspace.com/assets/images/storage-be14e1cff2cd5a4e4b10d3fb5ba457dd.gif)\\n\\n### Volume storage billing\\u200b\\n\\nThe amount of volume storage you have access to is dictated by your Gradient\\nsubscription tier as shown below. These storage limit are on a per notebook\\nbasis.\\n\\nAny storage over these limits will be charged at $.29/GB/month.\\n\\nThese charges are accrued hourly at the current usage of the bucket. For\\nexample, if a user with a free subscription, goes over the 5GB limit for 3\\ndays then the account will only be charged for the 3 days of usage over the\\nfree limit.\\n\\nSubscription| Volume Storage  \\n---|---  \\nFree| 5GB  \\nPro| 15GB  \\nGrowth| 50GB  \\n  \\n### Bucket storage\\u200b\\n\\nBuckets refers to long-term storage which is primarily used for offline\\nviewing of files and dataset versioning.\\n\\nExamples of bucket storage include:\\n\\n  *  **Offline data for Gradient Notebooks:** Files and datasets viewable offline in a notebook. In the offline view, buckets will store `.ipynb` files, `.md` files, any git tracked files, and any files included in the `.notebookinclude` file.\\n  *  **Versioned Datasets:** Each time a Gradient Dataset is versioned, these iterations of the dataset are stored within a bucket. For more information see [Versioned Data](https://docs.paperspace.com/gradient/data/#versioned-data).\\n\\ninfo\\n\\nTo avoid getting charged for bucket storage, delete Gradient Notebooks and\\nversioned datasets that may no longer be in use.\\n\\n![This diagram illustrates the lifecyle of storage while running a Gradient\\nNotebook.](https://docs.paperspace.com/assets/images/storage-lifecycle-b50c5a41090731bd3c794a23cfb37451.png)\\n\\n### Bucket storage billing\\u200b\\n\\nNo matter the subscription type all Gradient users receive 2GB free bucket\\nstorage.\\n\\nUsers who exceed that limit will be charged $0.29/GB/Month.\\n\\nThese charges are accrued hourly at the current usage of the bucket. For\\nexample, if a user goes over the 2GB limit for 3 days then the account will\\nonly be charged for the 3 days of usage over the free limit.\\n\\ndanger\\n\\nIf a user does not have a credit card associated with the team then there will\\nbe a strict cap at 2GB and could lead to a failed notebook teardown.\\n\\n[PreviousRuntimes](https://docs.paperspace.com/gradient/notebooks/runtimes)[NextMachines](https://docs.paperspace.com/gradient/notebooks/machines)\\n\\n![](https://t.co/i/adsct?bci=3&eci=2&event_id=c3c15a41-5e1b-4b29-9d4a-251ccc7c050a&events=%5B%5B%22pageview%22%2C%7B%7D%5D%5D&integration=advertiser&p_id=Twitter&p_user_id=0&pl_id=7b256d00-405d-4dea-8191-6f39a9fea9ff&tw_document_href=https%3A%2F%2Fdocs.paperspace.com%2Fgradient%2Fnotebooks%2Fnotebook-\\nstorage%2F&tw_iframe_status=0&tw_order_quantity=0&tw_sale_amount=0&txn_id=nyjr5&type=javascript&version=2.3.29)![](https://analytics.twitter.com/i/adsct?bci=3&eci=2&event_id=c3c15a41-5e1b-4b29-9d4a-251ccc7c050a&events=%5B%5B%22pageview%22%2C%7B%7D%5D%5D&integration=advertiser&p_id=Twitter&p_user_id=0&pl_id=7b256d00-405d-4dea-8191-6f39a9fea9ff&tw_document_href=https%3A%2F%2Fdocs.paperspace.com%2Fgradient%2Fnotebooks%2Fnotebook-\\nstorage%2F&tw_iframe_status=0&tw_order_quantity=0&tw_sale_amount=0&txn_id=nyjr5&type=javascript&version=2.3.29)\\n\\n', metadata={'source': 'documents\\\\user_1.md'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "def load_documents(data_dir:str) -> List[Document]:\n",
    "    md_files = list(map(str, pathlib.Path(data_dir).glob(\"*.md\")))\n",
    "    documents = [\n",
    "        TextLoader(file_path=file_path , encoding=\"utf8\").load()[0] for file_path in md_files\n",
    "    ]    \n",
    "    print(md_files)\n",
    "    return documents\n",
    "\n",
    "load_documents(\"documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdfb337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def chunk_documents(\n",
    "    documents: List[Document], chunk_size: int = 600, chunk_overlap=200\n",
    ") -> List[Document]:\n",
    "    \"\"\"Split documents into chunks\n",
    "\n",
    "    Args:\n",
    "        documents (List[Document]): A list of documents to split into chunks\n",
    "        chunk_size (int, optional): The size of each chunk. Defaults to 500.\n",
    "        chunk_overlap (int, optional): The number of tokens to overlap between chunks. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: A list of chunked documents.\n",
    "    \"\"\"\n",
    "    print(\"Before chunking \" , documents)\n",
    "    markdown_text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    split_documents = markdown_text_splitter.split_documents(documents)\n",
    "    return split_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff276bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunk_documents) -> Qdrant:\n",
    "\n",
    "    embedding_function = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "\n",
    "\n",
    "    # user_vector_store_path = os.path.join(vector_store_path, user_key)\n",
    "\n",
    "\n",
    "    # vector_store = Chroma.from_documents(\n",
    "    #     documents=documents,\n",
    "    #     embedding=embedding_function,\n",
    "    #     persist_directory=vector_store_path,\n",
    "    # )\n",
    "    #\n",
    "    # return vector_store\n",
    "\n",
    "    print(\"Docs \" , chunk_documents)\n",
    "\n",
    "    vector_store = Qdrant.from_documents(\n",
    "        chunk_documents,\n",
    "        embedding_function,\n",
    "        url=url,\n",
    "        prefer_grpc=True,\n",
    "        api_key=api_key_q,\n",
    "        collection_name=\"my_documents\",\n",
    "    )\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5c128888fdb20",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed0f2794082f406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T11:51:23.275222Z",
     "start_time": "2023-09-22T11:51:23.271278Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def ingest_data():\n",
    "\n",
    "    chunk_size = 600\n",
    "\n",
    "    chunk_overlap = 200\n",
    "    \n",
    "    # Load the documents\n",
    "    documents = load_documents(doc_dir)\n",
    "    print(\"Load Documents \" , documents)\n",
    "    \n",
    "    # Split the documents into chunks\n",
    "    chunked_documents = chunk_documents(documents, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Create the vector store with the chunked documents\n",
    "    vector_store = create_vector_store(chunked_documents)\n",
    "    return documents, vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727a870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Documents  []\n",
      "Before chunking  []\n",
      "Docs  []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ayusi\\OneDrive\\Desktop\\Hackathon-Projects\\Autonomous Agent\\autonomous_app\\LatestFlow.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ingest_data()\n",
      "\u001b[1;32mc:\\Users\\ayusi\\OneDrive\\Desktop\\Hackathon-Projects\\Autonomous Agent\\autonomous_app\\LatestFlow.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m chunked_documents \u001b[39m=\u001b[39m chunk_documents(documents, chunk_size, chunk_overlap)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Create the vector store with the chunked documents\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m vector_store \u001b[39m=\u001b[39m create_vector_store(chunked_documents)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m documents, vector_store\n",
      "\u001b[1;32mc:\\Users\\ayusi\\OneDrive\\Desktop\\Hackathon-Projects\\Autonomous Agent\\autonomous_app\\LatestFlow.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# user_vector_store_path = os.path.join(vector_store_path, user_key)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# return vector_store\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDocs \u001b[39m\u001b[39m\"\u001b[39m , chunk_documents)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m vector_store \u001b[39m=\u001b[39m Qdrant\u001b[39m.\u001b[39;49mfrom_documents(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     chunk_documents,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     embedding_function,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     prefer_grpc\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     api_key\u001b[39m=\u001b[39;49mapi_key_q,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     collection_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy_documents\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m vector_store\n",
      "File \u001b[1;32mc:\\Users\\ayusi\\Envs\\auto_vinod\\lib\\site-packages\\langchain\\vectorstores\\base.py:419\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m texts \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mpage_content \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[0;32m    418\u001b[0m metadatas \u001b[39m=\u001b[39m [d\u001b[39m.\u001b[39mmetadata \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m documents]\n\u001b[1;32m--> 419\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[39m=\u001b[39mmetadatas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ayusi\\Envs\\auto_vinod\\lib\\site-packages\\langchain\\vectorstores\\qdrant.py:1091\u001b[0m, in \u001b[0;36mQdrant.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, location, url, port, grpc_port, prefer_grpc, https, api_key, prefix, timeout, host, path, collection_name, distance_func, content_payload_key, metadata_payload_key, vector_name, batch_size, shard_number, replication_factor, write_consistency_factor, on_disk_payload, hnsw_config, optimizers_config, wal_config, quantization_config, init_from, on_disk, force_recreate, **kwargs)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    952\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_texts\u001b[39m(\n\u001b[0;32m    953\u001b[0m     \u001b[39mcls\u001b[39m: Type[Qdrant],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    986\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    987\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Qdrant:\n\u001b[0;32m    988\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Construct Qdrant wrapper from a list of texts.\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \n\u001b[0;32m    990\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[39m            qdrant = Qdrant.from_texts(texts, embeddings, \"localhost\")\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1091\u001b[0m     qdrant \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_construct_instance(\n\u001b[0;32m   1092\u001b[0m         texts,\n\u001b[0;32m   1093\u001b[0m         embedding,\n\u001b[0;32m   1094\u001b[0m         location,\n\u001b[0;32m   1095\u001b[0m         url,\n\u001b[0;32m   1096\u001b[0m         port,\n\u001b[0;32m   1097\u001b[0m         grpc_port,\n\u001b[0;32m   1098\u001b[0m         prefer_grpc,\n\u001b[0;32m   1099\u001b[0m         https,\n\u001b[0;32m   1100\u001b[0m         api_key,\n\u001b[0;32m   1101\u001b[0m         prefix,\n\u001b[0;32m   1102\u001b[0m         timeout,\n\u001b[0;32m   1103\u001b[0m         host,\n\u001b[0;32m   1104\u001b[0m         path,\n\u001b[0;32m   1105\u001b[0m         collection_name,\n\u001b[0;32m   1106\u001b[0m         distance_func,\n\u001b[0;32m   1107\u001b[0m         content_payload_key,\n\u001b[0;32m   1108\u001b[0m         metadata_payload_key,\n\u001b[0;32m   1109\u001b[0m         vector_name,\n\u001b[0;32m   1110\u001b[0m         shard_number,\n\u001b[0;32m   1111\u001b[0m         replication_factor,\n\u001b[0;32m   1112\u001b[0m         write_consistency_factor,\n\u001b[0;32m   1113\u001b[0m         on_disk_payload,\n\u001b[0;32m   1114\u001b[0m         hnsw_config,\n\u001b[0;32m   1115\u001b[0m         optimizers_config,\n\u001b[0;32m   1116\u001b[0m         wal_config,\n\u001b[0;32m   1117\u001b[0m         quantization_config,\n\u001b[0;32m   1118\u001b[0m         init_from,\n\u001b[0;32m   1119\u001b[0m         on_disk,\n\u001b[0;32m   1120\u001b[0m         force_recreate,\n\u001b[0;32m   1121\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   1122\u001b[0m     )\n\u001b[0;32m   1123\u001b[0m     qdrant\u001b[39m.\u001b[39madd_texts(texts, metadatas, ids, batch_size)\n\u001b[0;32m   1124\u001b[0m     \u001b[39mreturn\u001b[39;00m qdrant\n",
      "File \u001b[1;32mc:\\Users\\ayusi\\Envs\\auto_vinod\\lib\\site-packages\\langchain\\vectorstores\\qdrant.py:1349\u001b[0m, in \u001b[0;36mQdrant._construct_instance\u001b[1;34m(cls, texts, embedding, location, url, port, grpc_port, prefer_grpc, https, api_key, prefix, timeout, host, path, collection_name, distance_func, content_payload_key, metadata_payload_key, vector_name, shard_number, replication_factor, write_consistency_factor, on_disk_payload, hnsw_config, optimizers_config, wal_config, quantization_config, init_from, on_disk, force_recreate, **kwargs)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[39m# Just do a single quick embedding to get vector size\u001b[39;00m\n\u001b[0;32m   1348\u001b[0m partial_embeddings \u001b[39m=\u001b[39m embedding\u001b[39m.\u001b[39membed_documents(texts[:\u001b[39m1\u001b[39m])\n\u001b[1;32m-> 1349\u001b[0m vector_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(partial_embeddings[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m   1350\u001b[0m collection_name \u001b[39m=\u001b[39m collection_name \u001b[39mor\u001b[39;00m uuid\u001b[39m.\u001b[39muuid4()\u001b[39m.\u001b[39mhex\n\u001b[0;32m   1351\u001b[0m distance_func \u001b[39m=\u001b[39m distance_func\u001b[39m.\u001b[39mupper()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "ingest_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca9e279c9da6bf9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loading the Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "954d09f9e0785566",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:03:24.330332Z",
     "start_time": "2023-09-22T12:03:24.324558Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c425a8805b441667",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T12:02:49.881401Z",
     "start_time": "2023-09-22T12:02:49.860929Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading documents/user_1.md",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ayusi\\Envs\\auto_vinod\\lib\\site-packages\\langchain\\document_loaders\\text.py:40\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_path, encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     41\u001b[0m         text \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n",
      "\u001b[1;31mLookupError\u001b[0m: unknown encoding: **/*.md",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ayusi\\OneDrive\\Desktop\\Hackathon-Projects\\Autonomous Agent\\autonomous_app\\LatestFlow.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     dl \u001b[39m=\u001b[39m TextLoader(directory, \u001b[39m\"\u001b[39m\u001b[39m**/*.md\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m dl\u001b[39m.\u001b[39mload()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m documents \u001b[39m=\u001b[39m find_md_files(\u001b[39m'\u001b[39;49m\u001b[39mdocuments/user_1.md\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mlen\u001b[39m(documents)\n",
      "\u001b[1;32mc:\\Users\\ayusi\\OneDrive\\Desktop\\Hackathon-Projects\\Autonomous Agent\\autonomous_app\\LatestFlow.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mFind all markdown files in a directory and return a LangChain Document\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dl \u001b[39m=\u001b[39m TextLoader(directory, \u001b[39m\"\u001b[39m\u001b[39m**/*.md\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayusi/OneDrive/Desktop/Hackathon-Projects/Autonomous%20Agent/autonomous_app/LatestFlow.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dl\u001b[39m.\u001b[39;49mload()\n",
      "File \u001b[1;32mc:\\Users\\ayusi\\Envs\\auto_vinod\\lib\\site-packages\\langchain\\document_loaders\\text.py:56\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError loading \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m---> 56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError loading \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m     58\u001b[0m metadata \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_path}\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m [Document(page_content\u001b[39m=\u001b[39mtext, metadata\u001b[39m=\u001b[39mmetadata)]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading documents/user_1.md"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5c786aee0f7a4c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
